


\section{Reasoning with Discourse Markers}
We inspect the trained model and find the discourse markers which have the greatest impact for each question type. Our model tends to rely on different discourse markers in different reasoning steps. In Table~\ref{tab:disc_phrase}, we show the discourse markers used in the first two reasoning steps. As we can see, the discourse markers used in the first step are quite tailored to the question type while those in the second step are only loosely related. This shows that our model can navigate to the most relevant discourse marker very quickly, typically within one step. Next we analyze how the model chooses the focus to move based on discourse markers and the location of the target word for type 2 action. From the second to the fourth columns in Table~\ref{tab:disc_phrase} show which direction we should move to at each discourse marker. For instance, to answer a ``why'' question, if the current discourse marker we are looking at is ``due to'', then the model expects the target word and the explanation part (the answer we are looking for) to appear on opposite sides of the marker, which matches our intuition (\textit{i.e.} \textit{target word} due to \textit{answer}). The fourth column shows some discourse markers depend on context to select side to focus.




\textbf{Argumentative Discourse Structures.}
%One of the argument component classification tasks is to find a sentence in an essay that can serve as the major claim of the essay. 
In this section, we apply the deep reasoning model (DRM) to the task of understanding the persuasive essays. %Specifically, we aim to extract the major claims from an essay dataset by leveraging the knowledge encoded by DRM which was trained on a different dataset.  
Argument mining tasks include finding a sentence in an essay that can serve as the major claim and identifying its support arguments \cite{DBLP:conf/lrec/ReedPRM08}. Previous work \cite{DBLP:conf/emnlp/StabG14} has shown that both content and discourse are critical for understanding argument structures. Claims in persuasive essays heavily rely on the phrases that signal beliefs (\emph{e.g.} ``in my opinion").  

\cite{DBLP:conf/coling/StabG14} establishes gold-standard argument structure labels on a corpus of 90 persuasive essays. Each of these essays is associated with a topic, for instance \textit{``government and education"}. 
%This corpus contains annotations of argument components and argumentative relations at clause level. 
In each essay, one clause is labeled as a major claim, for example \textit{``government should pay for the university fees"}. Other clauses are labeled with support or non-support to the major claim, for example a supportive argument is \textit{``Students would be able to focus more on their studies rather than worrying about how to scrape together enough funds for each upcoming school term."} 

%https://arxiv.org/pdf/1704.07203.pdf


\textbf{Major claim extraction task.} Our DRM model, trained on the question answering dataset~\textsc{QAMR},  takes a short paragraph and a question as input, and predicts a short text segment as output. A whole essay can be too long for the DRM, but on this dataset the major claim always appear in the first paragraph, so we only feed the first paragraph of the essay, together with a made-up question concatenating \textit{``What is the opinion on"} to the essay's topic. For example, a question like \textit{``What is the opinion on government and education?"}. We then take the output segment of the DRM model and find the corresponding clause by highest overlap (essays are given segmented into clauses). 

% We try to keep the extracted major claim sentence as complete as possible by discarding certain discourse markers (\emph{e.g.} ``to") that may split a sentence into multiple short phrases. We note, however, our solution is not perfect, and some major claim sentences are still not kept complete. %It also makes sure that the correct major claim would not be split into multiple phrases by discourse markers. %After these filtering, we find that 74 out of 90 essays contain a complete major claim. For the rest of essays, we still consider them in our test dataset, and we will show more details about prediction results below.
We apply the trained DRM to all 90 essays in this dataset. For comparison, we take the SVM-based classifier proposed in \cite{DBLP:conf/emnlp/StabG14}, trained on many linguistic features. Our DRM outperforms the SVM method in terms of accuracy (0.489 \emph{vs.} 0.422). In particular, the DRM finds the most indicative of the major claims discourse markers  are: ``that" (\emph{e.g.} ``I strongly believe that..."), ``however", ``therefore", etc.




\textbf{Support classification task.}  Furthermore, we test whether the model's predictions can be utilized to predict the argumentative relations between clauses and the major claim (support vs. non-support). Our contribution is an addition of two binary features obtained from DRM, to the set of linguistic binary features extracted by \cite{DBLP:conf/emnlp/StabG14}: (1) whether the clause contains discourse markers on the path to identifying the major claim; (2) whether the clause is on the same side with the final segment after the first selection by using DRM model. Intuitively, the reasoning path learned by DRM model implies support relations. The DRM features on clause support/not improve the accuracy from 0.90 to 0.93.

%That's because these words appear in sentences like ``I strongly believe that...", 
%\cheng{give examples} %More examples can be found in the supplementary materials.

